# src/train.py

"""
í•™ìŠµ ë° í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

- train_epoch: ë‹¨ì¼ ì—í­ ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµ
- evaluate: ëª¨ë¸ì„ í‰ê°€í•˜ì—¬ Recall@K ë° NDCG@K ê³„ì‚°
"""

import torch 
from torch.utils.data import DataLoader
from tqdm import tqdm
from src.models.mbgcn import MBGCN
from src.data.preparation import (
    load_parquet_data,
    filter_users,
    split_train_test,
    build_user_item_matrices,
    build_item_item_matrices_transpose,
    build_user_behavior_dict
)
from src.loss import bpr_loss

# def train(
#     model: MBGCN,
#     train_loader: DataLoader,
#     optimizer: torch.optim.Optimizer,
#     device: torch.device,
#     beta: float
# ):
#     """
#     ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜.
    
#     Args:
#         model (MBGCN): í•™ìŠµí•  ëª¨ë¸
#         train_loader (DataLoader): í›ˆë ¨ ë°ì´í„° ë¡œë”
#         optimizer (Optimizer): ì˜µí‹°ë§ˆì´ì €
#         device (torch.device): ë””ë°”ì´ìŠ¤
#         beta (float): L2 ì •ê·œí™” ê³„ìˆ˜
#     """
#     model.train() # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
#     total_loss = 0.0
#     num_batches = len(train_loader)
    
#     # progress bar ì„¤ì •
#     progress = tqdm(train_loader, desc="Training ğŸ‹ï¸â€â™‚ï¸", leave=False)
    
#     for batch_idx, batch in enumerate(progress):
#         # ë°°ì¹˜ ë°ì´í„° 
#         user_ids = batch["user"].squeeze().to(device)         # shape: (batch_size,)
#         pos_item_ids = batch["pos_item"].squeeze().to(device) # shape: (batch_size,)
#         neg_item_ids = batch["neg_item"].to(device) # shape: (batch_size, neg_size)
        
#         batch_size, neg_size = neg_item_ids.size()
        
#         # ì‚¬ìš©ì IDsë¥¼ (batch_size * neg_size,) í˜•íƒœë¡œ í™•ì¥
#         user_ids_expanded = user_ids.unsqueeze(1).repeat(1, neg_size).view(-1)  # shape=(batch_size * neg_size,)
#         neg_item_ids_flat = neg_item_ids.view(-1)                            # shape=(batch_size * neg_size,)
        
#         # ëª¨ë¸ì„ í†µí•´ ë¶€ì • ìƒ˜í”Œ ì ìˆ˜ ê³„ì‚°
#         neg_scores = model(user_ids_expanded, neg_item_ids_flat)            # shape=(batch_size * neg_size,)
        
#         # ëª¨ë¸ì„ í†µí•´ ê¸ì • ìƒ˜í”Œ ì ìˆ˜ ê³„ì‚°
#         pos_scores = model(user_ids, pos_item_ids)                          # shape=(batch_size,)
#         pos_scores_expanded = pos_scores.unsqueeze(1).repeat(1, neg_size).view(-1)  # shape=(batch_size * neg_size,)
        
#         optimizer.zero_grad() # ì˜µí‹°ë§ˆì´ì €ì˜ ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        
#         # ë””ë²„ê·¸: í…ì„œ í¬ê¸° ì¶œë ¥
#         # print(f"Batch {batch_idx}:")
#         # print(f"  user_ids shape: {user_ids.shape}")               # (batch_size,)
#         # print(f"  pos_item_ids shape: {pos_item_ids.shape}")       # (batch_size,)
#         # print(f"  neg_item_ids shape: {neg_item_ids.shape}")       # (batch_size, neg_size)
#         # print(f"  user_ids_expanded shape: {user_ids_expanded.shape}")  # (batch_size * neg_size,)
#         # print(f"  neg_item_ids_flat shape: {neg_item_ids_flat.shape}")  # (batch_size * neg_size,)
#         # print(f"  pos_scores shape: {pos_scores.shape}")           # (batch_size,)
#         # print(f"  pos_scores_expanded shape: {pos_scores_expanded.shape}")  # (batch_size * neg_size,)
#         # print(f"  neg_scores shape: {neg_scores.shape}")           # (batch_size * neg_size,)
        
#         # BPR Loss ê³„ì‚°
#         try:
#             loss = bpr_loss(pos_scores_expanded, neg_scores, model, beta)
#         except Exception as e:
#             print(f"Error in batch {batch_idx}: {e}")
#             raise
        
#         # ì—­ì „íŒŒ ë° ìµœì í™”
#         loss.backward()
#         optimizer.step()

#         total_loss += loss.item()
#         progress.set_postfix(loss=loss.item())  # í˜„ì¬ ë°°ì¹˜ì˜ ì†ì‹¤ í‘œì‹œ

#     avg_loss = total_loss / num_batches
#     return avg_loss

def train_one_epoch(
    model: MBGCN,
    train_loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
    beta: float
):
    """
    Epoch ë‹¨ìœ„ í•™ìŠµ
    1) model.encode() => ì „ì²´ ì„ë² ë”© 1íšŒ ê³„ì‚° (ìŠ¤íŒŒìŠ¤ ì—°ì‚°)
    2) ê° ë¯¸ë‹ˆë°°ì¹˜ë³„ë¡œ forward (dot-product ê¸°ë°˜), BPR Loss
    """
    model.train()
    total_loss = 0.0
    num_batches = len(train_loader)
    
    # 1) ë©”ì‹œì§€ íŒ¨ì‹± (Epoch-level)
    model.embedding_cached.fill_(0)  # í˜¹ì‹œ ì´ë¯¸ 1ë¡œ ë˜ì–´ìˆë‹¤ë©´ ì´ˆê¸°í™”
    model.encode()                   # => ìºì‹±ëœ ì„ë² ë”©(user_latent, item_latent, s_item_list)
    
    progress = tqdm(train_loader, desc="Training ğŸ‹ï¸â€â™‚ï¸", leave=False)
    for batch_idx, batch in enumerate(progress):
        user_ids = batch["user"].squeeze().to(device)
        pos_item_ids = batch["pos_item"].squeeze().to(device)
        neg_item_ids = batch["neg_item"].to(device)  
        
        batch_size, neg_size = neg_item_ids.size()
        user_ids_expanded = user_ids.unsqueeze(1).repeat(1, neg_size).view(-1)
        neg_item_ids_flat = neg_item_ids.view(-1)
        
        # forward
        pos_scores = model(user_ids, pos_item_ids)  # (batch_size,)
        neg_scores = model(user_ids_expanded, neg_item_ids_flat)  # (batch_size*neg_size,)

        pos_scores_expanded = pos_scores.unsqueeze(1).repeat(1, neg_size).view(-1)
        
        optimizer.zero_grad()
        loss = bpr_loss(pos_scores_expanded, neg_scores, model, beta)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        progress.set_postfix(loss=f"{loss.item():.4f}")
    
    avg_loss = total_loss / num_batches
    return avg_loss